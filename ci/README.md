# Simple Continuous Integration

Дизайн максимально простой. Думал, что смогу в конце сделать его хоть чуть-чуть приятным, но в итоге добавил только разноцветные индикаторы результатам сборки.

В приложении есть сервер (далее так же - сервер) и произвольное количество агентов-серверов (далее - просто агент).

Из-за асинхронности и не до конца продуманной архитектуры возможны ошибки при одновременной работе нескольких агентов и отключении части из них, хотя все обнаруженные баги мне удалось найти и исправить. На моих глазах приложение работало корректно. Пожалуй, это единственная домашка, которой я доволен :)

## Агенты

### Запуск

```sh
$ npm i
$ npm start
```

### Об агентах

* Я решил для каждого агента делать свой сервер, поскольку так хотя бы понятно, как "убить" агент-процесс (точнее, конечно, не процесс, а сам агент-сервер). Если бы у нас был один сервер для всех агентов, то тогда бы ответственность за работу по утилизации агентов переходила бы с сервера `/server` на общий для всех агентов сервер `/agent`. Но по условию мне показалось, что управлением агентами должен заниматься именно `/server`
* Старт агентов: файл `/agent/index.js` запускает n-ое количество агентов через цикл. Список агентов задается через конфиг в файле `/agent/config.js`
* Предполагается, что один агент выполняет одну задачу по сборке. Технически ограничение со стороны агента не сделано: можно отправить запрос и заставить агента выполнять еще один дочерний процесс. Однако такое ограничение есть со стороны сервера: если агент в списке агентов у сервера имеет флаг `isBusy` равный `true`, то ему запрос на сборку не отправится
* Агенты могут выполнять тестовое отключение, чтобы могла быть проверена корректность работы и агента, и сервера при ситуации, когда существуют неполадки с сетью или агент или сервер упали (вывод логов со стороны агента, а также вывод логов, удаление агента из списка и изменение статуса у соответствующей сборки, если агент был в работе, со стороны сервера). Задать таймаут до отключения отдельного агента можно в конфиге `/agent/config.js` в поле `closeConnectionMs` соответствующего агента
* Агенты после принятия сборочной команды скачивают репозиторий в папку `/agent/buildRepos/:номер_сборки`, выполняют в ней саму сборочную команду и затем удаляют папку. Если произошло аварийное отключение агента, то папки с репозиториями сохранятся, но заново при старте скрипта `/agent/index.js` папка `buildRepos` будет удалена

## Сервер

### Запуск

```sh
$ npm i
$ npm start
```

### О сервере
* Старт сервера происходит в файле `/server/index.js`
* Большая часть логики сервера хранится в контроллерах:
  * `/server/controllers/AgentsController`: отвечает за работу агентов, которые регистрируются на сервере через ручку в `/server/routes/notifyAgent`. Контроллер агентов 1) регистрирует агентов в методе `addAgent`, добавляя их в соответствующий список; 2) отдает задание на сборку какому-либо из агентов (или ставит задание в очередь, если все агенты заняты) в методе `runTask`; 3) завершает работу агента по сборке, когда агент-сервер обратился по ручке в `/server/routes/notifyBuildResultHandler`; 4) пингует агентов в методе `pingAgents`: сеть и агенты должны быть живы на протяжении всего времени сборки (техническое ограничение моего приложения), иначе, если сервер обнаружит, что агент не пингуется, он уберет агента из списка агентов, а также поставит статус `error` сборке, если агент находился в процессе выполнения сборки
  * `/server/controllers/BuildsController`: контроллер для работы с результатами сборок в файловой системе. Контроллер сборок 1) инициализирует новую сборку в методе `initNewBuild`, создавая папку с соответствующим уникальным номером; 2) отдает контроллер конкретной сборки в методе `get`; 3) отдает список контроллеров всех сборок в методе `getAll`
  * `/server/controllers/BuildController`: "база данных" для работы с результатами конкретной сборки в файловой системе. Контроллер позволяет для конкретной сборки записать и считать информацию о 1) хэше коммита, сборочной команде, статусе сборки в файле `info.txt`; 2) хостнейме и порте агента в файле `agent.txt`; 3) результате работы сборки, если она была успешной, в файле `stdout.txt`; 4) ошибке, если сборка не была успешной, в файле `stderr.txt`; 5) времени сборки, если сборки была завершена с успешным или неуспешным кодом, в файле `dates.txt`
* Информация о сборках лежит в папке `/server/builds`

Возможные статусы у сборок:
* `building` - сервер отдал сборочную команду в обработку одному из агентов
* `queued` - сервер добавил сборочную команду в очередь, поскольку все агенты в данный момент заняты выполнением других сборочных команд
* `built` - сборка была успешно выполнена
* `error` - произошла ошибка. Что могло произойти:
  * Ошибка в результате сборки: не прошли тесты или что угодно еще. Остальные сообщения ниже - "системные"
  * `No agents available` - на сервере нет зарегистрированных агентов
  * `The connection was lost` - соединение между сервером и агентом было потеряно. Это может произойти в двух случаях: 1) при пинговании агент не ответил; 2) при запуске сервера оставались сборки, которые были либо в выполнении, либо в очереди на выполнение 

## Требования
* Сервер должен генерировать уникальные номера сборок ✅. Каждой новой сборке дается следующее по счету число
* Сервер должен максимально утилизировать имеющихся агентов: думаю, что ✅. Что можно понимать под максимальной утилизацией: у сервера есть список агентов, каждый агент выполняет ровно одну сборочную задачу. Значит, агент либо занят, либо нет. Тогда задача сервера - отдавать задачу свободному агенту. Не знаю, что еще можно было учесть
* Сервер должен корректно обрабатывать ситуацию, когда агент прекратил работать между сборками ✅. Сервер каждые 5 секунд пингует всех агентов на предмет того, живы ли они (не важно, заняты сборкой или нет). Если агент прекратил работу между сборками, то сервер удаляет агента из списка агентов
* Сервер должен корректно обрабатывать ситуацию, когда агент прекратил работать в процессе выполнения сборки ✅. В этом случае сервер 1) удаляет агента из списка агентов; 2) меняет статус у сборки на `error` и в `stderr.txt` записывает сообщение `The connection was lost`. Возможна такая ситуация, что соединение на короткое время было потеряно, пинг это зафиксировал, но на деле агент все это время работал, и в конце отправил результат сборки по ручке `/notify_build_result`. Тогда сервер не примет результат сборки, поскольку, опять-таки, существует техническое ограничение, что соединение во время сборки должно быть установлено постоянно. Агент будет об этом оповещен, а затем прекратит свою работу
* Сервер должен корректно обрабатывать ситуацию, когда агенты не справляются с поступающими заявками ✅. Сервер добавляет сборочную команду в очередь. Команда начнет выполняться, если 1) какой-либо из агентов закончит работу с предыдущей сборочной командой и тогда ему будет отдана команда из очереди на выполнение в обработчике `/server/routes/notifyBuildResultHandler`; 2) на сервере зарегистрируется новый агент, у которого, конечно, еще нет никаких команд в выполнении (`/server/routes/notifyAgent`)
* Агент должен корректно обрабатывать ситуацию, когда при старте не смог соединиться с сервером ✅. Агент выводит сообщение в консоль о неудачной попытке соединения и отключается. Чуть более лучшая реализация это, наверное, дать возможность агенту попытаться подключиться к серверу n-раз и только после всех попыток отключиться, но я это не сделал. У агента нет информации по поводу работы сервера, он регистрируется на нем самостоятельно, сервер не отправляет никаких запросов к агенту до его регистрации, поскольку он просто не знает адрес, по которому расположен агент. Возможно, попытки подключения нужно разнести во времени (условно каждые 10 минут)
* Агент должен корректно обрабатывать ситуацию, когда при отправке результатов сборки не смог соединиться с сервером ✅. Агент выводит информацию о неудачной попытке соединения в консоль и отключается, предварительно удалив папку с репозиторием

## Инструменты

### Агент
Node-модули:
* **express** - нужен для работы агента-сервера
* **body-parser** - возможность считывания тела POST-запросов
* **axios** - необходим для отправки запросов к серверу
* **rimraf** - необходим для кроссплатформенного удаления папок с временно устанавливаемыми репозиториями

### Сервер
Node-модули:
* **express**, **body-parser**, **axios**, **rimraf** - для того же самого
* **tcp-ping** - необходим для пингования агентов